![Alt text](readme/llmevalforge_image.png)

# LLMEvalForge: Model Evaluation Framework

**LLMEvalForge** is a Python framework designed to automate the evaluation of Large Language Models (LLMs) in custom domains. It enables the creation of tailored test suites to assess models on specific tasks, domains, or ethical considerations. 


# Table of Contents

1. [Installation Instructions](#1-installation-instructions)
2. [Files Overview](#2-files-overview)
   - 2.1. [Configuration File: configs.yaml](#21-configuration-file-configsyaml)
   - 2.2. [Using the Web App](#22-using-the-web-app)
3. [Evaluation Tests](#3-evaluation-tests)


---

# 1. Installation Instructions

- **Python version required**: `python 3.10.12`

- **Dependencies**: Install required packages using

```bash
pip install -r requirements.txt
```

- **Running the web app**: Use Streamlit to run the web application
```bash
streamlit run app.py
```
(Refer to section 2.2 for detailed usage of the web app)

- **Running the Evaluation Pipeline**:

```bash
python main.py --config <path_to_config file: config.yaml> --output <directory to save results of test>
```


# 2. Files Overview

- **`main.py`**: Core script for running evaluations on language models. It orchestrates the entire process from loading configurations to executing tasks and saving results.

- **`configs.yaml`**: Configuration file specifying evaluation tasks, candidate models, and evaluator settings.

- **`app.py`**: Streamlit application providing a user interface for running model evaluations interactively, allowing real-time interaction and visualization of results.

- **`model_handler/evaluators.py`**: Contains the `run_evaluation` function, responsible for evaluating model responses, comparing them against expected outputs, and providing scores and feedback.

- **`task_handler/summarization.py`**: Contains the `run_summarization_task` function, which executes the summarization task, processes input data, and returns model outputs for evaluation.

- **`prompt_library/evaluator_prompt.yaml`**: Contains system and user prompts used by the evaluator.


## 2.1 Configuration File: `configs.yaml`

The configuration file includes:

- **`evaluation_tasks`**:  Lists tasks with their dataset files and parameters. Each task specifies a YAML file with examples to assess the model. The `run_eval` flag determines if the task requires evaluation of the candidate model's output by a judge-LLM.
It's important that the name of the task matches the base name of the python file with the task runner: in this instance `summarization.py`.

- **`candidate_model`**: Defines the model to be evaluated, including its module, class, and parameters. The configuration file specifies a model from the `langchain_community.llms` module, using the `Ollama` class. In our instance, the model used is `llama3.2:3b-instruct-fp16`, which is a 3-billion-parameter instruction-tuned version of the Llama model using half-precision (fp16) for efficiency. The `temperature` parameter is set to 0, meaning the model will produce deterministic outputs, and `num_predict` is set to 600, indicating the maximum token length for predictions.

- evaluator: Configures the evaluator model *aka* judge  model, including API key source, model parameters, and prompts. The evaluator uses a set of prompts stored in the `prompt_library/evaluator_prompts.yaml` file and applies the `NumericalScorer` to score the outputs. This setup facilitates structured evaluation and scoring of responses generated by the evaluator model based on predefined prompts.

<details>
<summary>config.yaml</summary>

```
evaluation_tasks:
  summarization:
    dataset_file: custom_tasks/summarization_ecommerce_tests.yaml
    run_eval: true
  instruction_following:
    dataset_file: custom_tasks/instruction_following_ecommerce_tests.yaml
    run_eval: true

candidate_model:
  module: langchain_community.llms
  class: Ollama
  params:
    model: llama3.2:3b-instruct-fp16
    temperature: 0
    num_predict: 600

evaluator:
  name: chatgpt_evaluator
  api_key_source: env  # Can be 'env' or 'file'
  model_params:
    model_name: gpt-4o
    temperature: 0
  prompt_file: prompt_library/evaluator_prompts.yaml  
  scorer: NumericalScorer
```
</details>

The config file can be customized with:

- appending more tests:
```json
evaluation_tasks:
  summarization:
    dataset_file: custom_tasks/summarization_ecommerce_tests.yaml
    run_eval: true
  instruction_following:
    dataset_file: custom_tasks/instruction_following_ecommerce_tests.yaml
    run_eval: true
  general_knowledge:
    dataset_file: custom_tasks/general_knowledge_ecommerce_tests.yaml
    run_eval: true
```

- new candidate models: **LLMEvalForge** only allows the use of models avaialable via *ollama* framework.


## 2.2 Using the Web App

The web application, built with Streamlit, offers a transparent, interactive approach to model evaluation, allowing users to select and customize individual tests or cases, manually execute the candidate model, and view outputs and evaluations step-by-step.

To run the webapp:

```bash
streamlit run app.py
```

Here's a breakdown of how it works:

![Alt text](readme/llmevalforge_tool.png)

<details>
  <summary>Click to see more</summary>

### A. Configuration Section (left panel)

- **`**Upload config YAML file`**: Upload a configuration file (`YAML` format) containing parameters and settings for the model evaluation.

- **Candidate Model**: The selected model to be evaluated is `llama3.2:3b-instruct-fp16`.

- **Select task file**: Choose the task type for evaluation, such as "summarization". Select specific test cases (*e.g.*, "Case 1") or use "random" for varied testing.


### B. Candidate Model Inference (right panel)

- **`System Prompt`**: The prompt guiding the candidate model's behavior, e.g., generating product descriptions.

- **`Instruction`**: Specifies the task for the model, such as generating a product description for a specific item.

- **Run Candidate Model**: Execute the model on the selected task, generating output (`model_output`) and displaying the `expected output`.

- **expected output**: Shows the ideal output the candidate model should generate.

### Output Evaluation

- **Run Evaluator Button**: Evaluate the quality of the generated output based on expected results, displaying scores and feedback from the model evaluator. 
</details>

# 3. Evaluation Tests

For detailed information about evaluation tests, refer to [task_info.md](custom_tasks/task_info.md)

---
# TODO 

[ ] edit ethical_reasoning test, common sense test and code the associated run_tasks
[ ] preserve sorting of keys in saved yaml after evaluation
[ ] enable judge-llm to be a local LLM or a different close-source (claude, grok)